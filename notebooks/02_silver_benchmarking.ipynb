{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b556a1-d44e-4fe0-8668-c548cfce38ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Volumes roots\n",
    "ROOT   = \"/Volumes/tabular/dataexpert/benchmarking_capstone\"\n",
    "BRONZE = f\"{ROOT}\"\n",
    "SILVER = f\"{ROOT}/silver\"\n",
    "\n",
    "# Bronze inputs\n",
    "TX_BRONZE_PATH   = f\"{BRONZE}/raw_transactions_daily\"\n",
    "CUST_BRONZE_PATH = f\"{BRONZE}/raw_customers\"\n",
    "\n",
    "# Silver outputs\n",
    "TX_SILVER_PATH   = f\"{SILVER}/transactions_clean\"\n",
    "CUST_SILVER_PATH = f\"{SILVER}/customers_clean\"\n",
    "\n",
    "# --- Widgets for flexibility ---\n",
    "dbutils.widgets.dropdown(\"WRITE_MODE\", \"append\", [\"append\",\"overwrite_full\",\"overwrite_range\"])\n",
    "dbutils.widgets.text(\"DATE_FROM\", \"\")   # e.g. 2024-03-01 (inclusive)\n",
    "dbutils.widgets.text(\"DATE_TO\", \"\")     # e.g. 2024-04-01 (exclusive); leave blank for open-ended\n",
    "\n",
    "WRITE_MODE = dbutils.widgets.get(\"WRITE_MODE\")\n",
    "DATE_FROM  = dbutils.widgets.get(\"DATE_FROM\").strip() or None\n",
    "DATE_TO    = dbutils.widgets.get(\"DATE_TO\").strip() or None\n",
    "\n",
    "# Shuffle for 60M+ scale; tune if needed\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1600\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a75b43-50f2-43bf-89b2-fd3bcd11fa2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote customers → /Volumes/tabular/dataexpert/benchmarking_capstone/silver/customers_clean\nRows: 200000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "cust_bronze = spark.read.format(\"delta\").load(CUST_BRONZE_PATH)\n",
    "\n",
    "cust_silver = (\n",
    "    cust_bronze\n",
    "      .dropDuplicates([\"customer_id\"])\n",
    "      .filter(F.col(\"customer_id\").isNotNull())\n",
    ")\n",
    "\n",
    "# Atomic full refresh of customers (dimension table is small)\n",
    "(cust_silver.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .save(CUST_SILVER_PATH))\n",
    "\n",
    "print(\"✅ Wrote customers →\", CUST_SILVER_PATH)\n",
    "print(\"Rows:\", spark.read.format(\"delta\").load(CUST_SILVER_PATH).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb08f319-484b-4acf-9aa8-8713a520cea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PARTITION OVERWRITE where ingest_day >= '2024-01-11' AND ingest_day < '2024-05-31' → /Volumes/tabular/dataexpert/benchmarking_capstone/silver/transactions_clean\n"
     ]
    }
   ],
   "source": [
    "if WRITE_MODE == \"append\":\n",
    "    # ✅ Normal daily ingestion\n",
    "    (tx_silver_df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"append\")\n",
    "       .partitionBy(\"ingest_day\")\n",
    "       .save(TX_SILVER_PATH))\n",
    "    print(\"✅ APPEND complete →\", TX_SILVER_PATH)\n",
    "\n",
    "elif WRITE_MODE == \"overwrite_full\":\n",
    "    # ✅ Atomic full refresh of entire table (same path, no rm/mv)\n",
    "    (tx_silver_df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"overwrite\")\n",
    "       .partitionBy(\"ingest_day\")\n",
    "       .save(TX_SILVER_PATH))\n",
    "    print(\"✅ FULL OVERWRITE complete →\", TX_SILVER_PATH)\n",
    "\n",
    "elif WRITE_MODE == \"overwrite_range\":\n",
    "    # ✅ Replace only the specified window (DATE_FROM/DATE_TO must define the range you’re writing)\n",
    "    if not DATE_FROM and not DATE_TO:\n",
    "        raise ValueError(\"overwrite_range requires DATE_FROM and/or DATE_TO\")\n",
    "    # Build Delta predicate string to match what we filtered\n",
    "    pred = []\n",
    "    if DATE_FROM: pred.append(f\"ingest_day >= '{DATE_FROM}'\")\n",
    "    if DATE_TO:   pred.append(f\"ingest_day < '{DATE_TO}'\")\n",
    "    replace_where = \" AND \".join(pred)\n",
    "\n",
    "    # Filter the DataFrame to match the replaceWhere condition\n",
    "    filtered_df = tx_silver_df.filter(replace_where)\n",
    "\n",
    "    (filtered_df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"replaceWhere\", replace_where)\n",
    "       .partitionBy(\"ingest_day\")\n",
    "       .save(TX_SILVER_PATH))\n",
    "    print(f\"✅ PARTITION OVERWRITE where {replace_where} → {TX_SILVER_PATH}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown WRITE_MODE: {WRITE_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57484fe-5c4d-400f-8eb4-d93db470c130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver tx rows: 270000000\nSilver cust rows: 200000\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ingest_day</th><th>count</th></tr></thead><tbody><tr><td>2024-01-11</td><td>1000000</td></tr><tr><td>2024-01-16</td><td>1000000</td></tr><tr><td>2024-01-31</td><td>1000000</td></tr><tr><td>2024-03-01</td><td>1000000</td></tr><tr><td>2024-03-02</td><td>1000000</td></tr><tr><td>2024-03-03</td><td>1000000</td></tr><tr><td>2024-03-04</td><td>1500000</td></tr><tr><td>2024-03-05</td><td>1500000</td></tr><tr><td>2024-03-06</td><td>1500000</td></tr><tr><td>2024-03-07</td><td>1500000</td></tr><tr><td>2024-03-08</td><td>1500000</td></tr><tr><td>2024-03-09</td><td>1500000</td></tr><tr><td>2024-03-10</td><td>1500000</td></tr><tr><td>2024-03-11</td><td>1500000</td></tr><tr><td>2024-03-12</td><td>1500000</td></tr><tr><td>2024-03-13</td><td>1500000</td></tr><tr><td>2024-03-14</td><td>1500000</td></tr><tr><td>2024-03-15</td><td>1500000</td></tr><tr><td>2024-03-16</td><td>1500000</td></tr><tr><td>2024-03-17</td><td>1500000</td></tr><tr><td>2024-03-18</td><td>1500000</td></tr><tr><td>2024-03-19</td><td>1500000</td></tr><tr><td>2024-03-20</td><td>1500000</td></tr><tr><td>2024-03-21</td><td>1500000</td></tr><tr><td>2024-03-22</td><td>1500000</td></tr><tr><td>2024-03-23</td><td>1500000</td></tr><tr><td>2024-03-24</td><td>1500000</td></tr><tr><td>2024-03-25</td><td>1500000</td></tr><tr><td>2024-03-26</td><td>1500000</td></tr><tr><td>2024-03-27</td><td>1500000</td></tr><tr><td>2024-03-28</td><td>1500000</td></tr><tr><td>2024-03-29</td><td>1500000</td></tr><tr><td>2024-03-30</td><td>1500000</td></tr><tr><td>2024-03-31</td><td>1500000</td></tr><tr><td>2024-04-01</td><td>1500000</td></tr><tr><td>2024-04-02</td><td>1500000</td></tr><tr><td>2024-04-03</td><td>1500000</td></tr><tr><td>2024-04-04</td><td>1500000</td></tr><tr><td>2024-04-05</td><td>1500000</td></tr><tr><td>2024-04-06</td><td>1500000</td></tr><tr><td>2024-04-07</td><td>1500000</td></tr><tr><td>2024-04-08</td><td>1500000</td></tr><tr><td>2024-04-09</td><td>1500000</td></tr><tr><td>2024-04-10</td><td>1500000</td></tr><tr><td>2024-04-11</td><td>1500000</td></tr><tr><td>2024-04-12</td><td>1500000</td></tr><tr><td>2024-04-13</td><td>1500000</td></tr><tr><td>2024-04-14</td><td>1500000</td></tr><tr><td>2024-04-15</td><td>1500000</td></tr><tr><td>2024-04-16</td><td>1500000</td></tr><tr><td>2024-04-17</td><td>1500000</td></tr><tr><td>2024-04-18</td><td>1500000</td></tr><tr><td>2024-04-19</td><td>1500000</td></tr><tr><td>2024-04-20</td><td>1500000</td></tr><tr><td>2024-04-21</td><td>1500000</td></tr><tr><td>2024-04-22</td><td>1500000</td></tr><tr><td>2024-04-23</td><td>1500000</td></tr><tr><td>2024-04-24</td><td>1500000</td></tr><tr><td>2024-04-25</td><td>1500000</td></tr><tr><td>2024-04-26</td><td>1500000</td></tr><tr><td>2024-04-27</td><td>1500000</td></tr><tr><td>2024-04-28</td><td>1500000</td></tr><tr><td>2024-04-29</td><td>1500000</td></tr><tr><td>2024-04-30</td><td>1500000</td></tr><tr><td>2024-05-01</td><td>1500000</td></tr><tr><td>2024-05-02</td><td>1500000</td></tr><tr><td>2024-05-03</td><td>1500000</td></tr><tr><td>2024-05-04</td><td>1500000</td></tr><tr><td>2024-05-05</td><td>1500000</td></tr><tr><td>2024-05-06</td><td>1500000</td></tr><tr><td>2024-05-07</td><td>1500000</td></tr><tr><td>2024-05-08</td><td>1500000</td></tr><tr><td>2024-05-09</td><td>1500000</td></tr><tr><td>2024-05-10</td><td>1500000</td></tr><tr><td>2024-05-11</td><td>1500000</td></tr><tr><td>2024-05-12</td><td>1500000</td></tr><tr><td>2024-05-13</td><td>1500000</td></tr><tr><td>2024-05-14</td><td>1500000</td></tr><tr><td>2024-05-15</td><td>1500000</td></tr><tr><td>2024-05-16</td><td>1500000</td></tr><tr><td>2024-05-17</td><td>1500000</td></tr><tr><td>2024-05-18</td><td>1500000</td></tr><tr><td>2024-05-19</td><td>1500000</td></tr><tr><td>2024-05-20</td><td>1500000</td></tr><tr><td>2024-05-21</td><td>1500000</td></tr><tr><td>2024-05-22</td><td>1500000</td></tr><tr><td>2024-05-23</td><td>1500000</td></tr><tr><td>2024-05-24</td><td>1500000</td></tr><tr><td>2024-05-25</td><td>1500000</td></tr><tr><td>2024-05-26</td><td>1500000</td></tr><tr><td>2024-05-27</td><td>1500000</td></tr><tr><td>2024-05-28</td><td>1500000</td></tr><tr><td>2024-05-29</td><td>1500000</td></tr><tr><td>2024-05-30</td><td>1500000</td></tr><tr><td>2024-06-04</td><td>1500000</td></tr><tr><td>2024-06-05</td><td>1500000</td></tr><tr><td>2024-06-06</td><td>1500000</td></tr><tr><td>2024-06-07</td><td>1500000</td></tr><tr><td>2024-06-08</td><td>1500000</td></tr><tr><td>2024-06-09</td><td>1500000</td></tr><tr><td>2024-06-10</td><td>1500000</td></tr><tr><td>2024-06-11</td><td>1500000</td></tr><tr><td>2024-06-12</td><td>1500000</td></tr><tr><td>2024-06-13</td><td>1500000</td></tr><tr><td>2024-06-14</td><td>1500000</td></tr><tr><td>2024-06-15</td><td>1500000</td></tr><tr><td>2024-06-16</td><td>1500000</td></tr><tr><td>2024-06-17</td><td>1500000</td></tr><tr><td>2024-06-18</td><td>1500000</td></tr><tr><td>2024-06-19</td><td>1500000</td></tr><tr><td>2024-06-20</td><td>1500000</td></tr><tr><td>2024-06-21</td><td>1500000</td></tr><tr><td>2024-06-22</td><td>1500000</td></tr><tr><td>2024-06-23</td><td>1500000</td></tr><tr><td>2024-06-24</td><td>1500000</td></tr><tr><td>2024-06-25</td><td>1500000</td></tr><tr><td>2024-06-26</td><td>1500000</td></tr><tr><td>2024-06-27</td><td>1500000</td></tr><tr><td>2024-06-28</td><td>1500000</td></tr><tr><td>2024-06-29</td><td>1500000</td></tr><tr><td>2024-06-30</td><td>1500000</td></tr><tr><td>2024-06-31</td><td>1500000</td></tr><tr><td>2024-07-01</td><td>1500000</td></tr><tr><td>2024-07-02</td><td>1500000</td></tr><tr><td>2024-07-03</td><td>1500000</td></tr><tr><td>2024-07-04</td><td>1500000</td></tr><tr><td>2024-07-05</td><td>1500000</td></tr><tr><td>2024-07-06</td><td>1500000</td></tr><tr><td>2024-07-07</td><td>1500000</td></tr><tr><td>2024-07-08</td><td>1500000</td></tr><tr><td>2024-07-09</td><td>1500000</td></tr><tr><td>2024-07-10</td><td>1500000</td></tr><tr><td>2024-07-11</td><td>1500000</td></tr><tr><td>2024-07-12</td><td>1500000</td></tr><tr><td>2024-07-13</td><td>1500000</td></tr><tr><td>2024-07-14</td><td>1500000</td></tr><tr><td>2024-07-15</td><td>1500000</td></tr><tr><td>2024-07-16</td><td>1500000</td></tr><tr><td>2024-07-17</td><td>1500000</td></tr><tr><td>2024-07-18</td><td>1500000</td></tr><tr><td>2024-07-19</td><td>1500000</td></tr><tr><td>2024-07-20</td><td>1500000</td></tr><tr><td>2024-07-21</td><td>1500000</td></tr><tr><td>2024-07-22</td><td>1500000</td></tr><tr><td>2024-07-23</td><td>1500000</td></tr><tr><td>2024-07-24</td><td>1500000</td></tr><tr><td>2024-07-25</td><td>1500000</td></tr><tr><td>2024-07-26</td><td>1500000</td></tr><tr><td>2024-07-27</td><td>1500000</td></tr><tr><td>2024-07-28</td><td>1500000</td></tr><tr><td>2024-07-29</td><td>1500000</td></tr><tr><td>2024-07-30</td><td>1500000</td></tr><tr><td>2024-08-01</td><td>1500000</td></tr><tr><td>2024-08-02</td><td>1500000</td></tr><tr><td>2024-08-03</td><td>1500000</td></tr><tr><td>2024-08-04</td><td>1500000</td></tr><tr><td>2024-08-05</td><td>1500000</td></tr><tr><td>2024-08-06</td><td>1500000</td></tr><tr><td>2024-08-07</td><td>1500000</td></tr><tr><td>2024-08-08</td><td>1500000</td></tr><tr><td>2024-08-09</td><td>1500000</td></tr><tr><td>2024-08-10</td><td>1500000</td></tr><tr><td>2024-08-11</td><td>1500000</td></tr><tr><td>2024-08-12</td><td>1500000</td></tr><tr><td>2024-08-13</td><td>1500000</td></tr><tr><td>2024-08-14</td><td>1500000</td></tr><tr><td>2024-08-15</td><td>1500000</td></tr><tr><td>2024-08-16</td><td>1500000</td></tr><tr><td>2024-08-17</td><td>1500000</td></tr><tr><td>2024-08-18</td><td>1500000</td></tr><tr><td>2024-08-19</td><td>1500000</td></tr><tr><td>2024-08-20</td><td>1500000</td></tr><tr><td>2024-08-21</td><td>1500000</td></tr><tr><td>2024-08-22</td><td>1500000</td></tr><tr><td>2024-08-23</td><td>1500000</td></tr><tr><td>2024-08-24</td><td>1500000</td></tr><tr><td>2024-08-25</td><td>1500000</td></tr><tr><td>2024-08-26</td><td>1500000</td></tr><tr><td>2024-08-27</td><td>1500000</td></tr><tr><td>2024-08-28</td><td>1500000</td></tr><tr><td>2024-08-29</td><td>1500000</td></tr><tr><td>2024-08-30</td><td>1500000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-01-11",
         1000000
        ],
        [
         "2024-01-16",
         1000000
        ],
        [
         "2024-01-31",
         1000000
        ],
        [
         "2024-03-01",
         1000000
        ],
        [
         "2024-03-02",
         1000000
        ],
        [
         "2024-03-03",
         1000000
        ],
        [
         "2024-03-04",
         1500000
        ],
        [
         "2024-03-05",
         1500000
        ],
        [
         "2024-03-06",
         1500000
        ],
        [
         "2024-03-07",
         1500000
        ],
        [
         "2024-03-08",
         1500000
        ],
        [
         "2024-03-09",
         1500000
        ],
        [
         "2024-03-10",
         1500000
        ],
        [
         "2024-03-11",
         1500000
        ],
        [
         "2024-03-12",
         1500000
        ],
        [
         "2024-03-13",
         1500000
        ],
        [
         "2024-03-14",
         1500000
        ],
        [
         "2024-03-15",
         1500000
        ],
        [
         "2024-03-16",
         1500000
        ],
        [
         "2024-03-17",
         1500000
        ],
        [
         "2024-03-18",
         1500000
        ],
        [
         "2024-03-19",
         1500000
        ],
        [
         "2024-03-20",
         1500000
        ],
        [
         "2024-03-21",
         1500000
        ],
        [
         "2024-03-22",
         1500000
        ],
        [
         "2024-03-23",
         1500000
        ],
        [
         "2024-03-24",
         1500000
        ],
        [
         "2024-03-25",
         1500000
        ],
        [
         "2024-03-26",
         1500000
        ],
        [
         "2024-03-27",
         1500000
        ],
        [
         "2024-03-28",
         1500000
        ],
        [
         "2024-03-29",
         1500000
        ],
        [
         "2024-03-30",
         1500000
        ],
        [
         "2024-03-31",
         1500000
        ],
        [
         "2024-04-01",
         1500000
        ],
        [
         "2024-04-02",
         1500000
        ],
        [
         "2024-04-03",
         1500000
        ],
        [
         "2024-04-04",
         1500000
        ],
        [
         "2024-04-05",
         1500000
        ],
        [
         "2024-04-06",
         1500000
        ],
        [
         "2024-04-07",
         1500000
        ],
        [
         "2024-04-08",
         1500000
        ],
        [
         "2024-04-09",
         1500000
        ],
        [
         "2024-04-10",
         1500000
        ],
        [
         "2024-04-11",
         1500000
        ],
        [
         "2024-04-12",
         1500000
        ],
        [
         "2024-04-13",
         1500000
        ],
        [
         "2024-04-14",
         1500000
        ],
        [
         "2024-04-15",
         1500000
        ],
        [
         "2024-04-16",
         1500000
        ],
        [
         "2024-04-17",
         1500000
        ],
        [
         "2024-04-18",
         1500000
        ],
        [
         "2024-04-19",
         1500000
        ],
        [
         "2024-04-20",
         1500000
        ],
        [
         "2024-04-21",
         1500000
        ],
        [
         "2024-04-22",
         1500000
        ],
        [
         "2024-04-23",
         1500000
        ],
        [
         "2024-04-24",
         1500000
        ],
        [
         "2024-04-25",
         1500000
        ],
        [
         "2024-04-26",
         1500000
        ],
        [
         "2024-04-27",
         1500000
        ],
        [
         "2024-04-28",
         1500000
        ],
        [
         "2024-04-29",
         1500000
        ],
        [
         "2024-04-30",
         1500000
        ],
        [
         "2024-05-01",
         1500000
        ],
        [
         "2024-05-02",
         1500000
        ],
        [
         "2024-05-03",
         1500000
        ],
        [
         "2024-05-04",
         1500000
        ],
        [
         "2024-05-05",
         1500000
        ],
        [
         "2024-05-06",
         1500000
        ],
        [
         "2024-05-07",
         1500000
        ],
        [
         "2024-05-08",
         1500000
        ],
        [
         "2024-05-09",
         1500000
        ],
        [
         "2024-05-10",
         1500000
        ],
        [
         "2024-05-11",
         1500000
        ],
        [
         "2024-05-12",
         1500000
        ],
        [
         "2024-05-13",
         1500000
        ],
        [
         "2024-05-14",
         1500000
        ],
        [
         "2024-05-15",
         1500000
        ],
        [
         "2024-05-16",
         1500000
        ],
        [
         "2024-05-17",
         1500000
        ],
        [
         "2024-05-18",
         1500000
        ],
        [
         "2024-05-19",
         1500000
        ],
        [
         "2024-05-20",
         1500000
        ],
        [
         "2024-05-21",
         1500000
        ],
        [
         "2024-05-22",
         1500000
        ],
        [
         "2024-05-23",
         1500000
        ],
        [
         "2024-05-24",
         1500000
        ],
        [
         "2024-05-25",
         1500000
        ],
        [
         "2024-05-26",
         1500000
        ],
        [
         "2024-05-27",
         1500000
        ],
        [
         "2024-05-28",
         1500000
        ],
        [
         "2024-05-29",
         1500000
        ],
        [
         "2024-05-30",
         1500000
        ],
        [
         "2024-06-04",
         1500000
        ],
        [
         "2024-06-05",
         1500000
        ],
        [
         "2024-06-06",
         1500000
        ],
        [
         "2024-06-07",
         1500000
        ],
        [
         "2024-06-08",
         1500000
        ],
        [
         "2024-06-09",
         1500000
        ],
        [
         "2024-06-10",
         1500000
        ],
        [
         "2024-06-11",
         1500000
        ],
        [
         "2024-06-12",
         1500000
        ],
        [
         "2024-06-13",
         1500000
        ],
        [
         "2024-06-14",
         1500000
        ],
        [
         "2024-06-15",
         1500000
        ],
        [
         "2024-06-16",
         1500000
        ],
        [
         "2024-06-17",
         1500000
        ],
        [
         "2024-06-18",
         1500000
        ],
        [
         "2024-06-19",
         1500000
        ],
        [
         "2024-06-20",
         1500000
        ],
        [
         "2024-06-21",
         1500000
        ],
        [
         "2024-06-22",
         1500000
        ],
        [
         "2024-06-23",
         1500000
        ],
        [
         "2024-06-24",
         1500000
        ],
        [
         "2024-06-25",
         1500000
        ],
        [
         "2024-06-26",
         1500000
        ],
        [
         "2024-06-27",
         1500000
        ],
        [
         "2024-06-28",
         1500000
        ],
        [
         "2024-06-29",
         1500000
        ],
        [
         "2024-06-30",
         1500000
        ],
        [
         "2024-06-31",
         1500000
        ],
        [
         "2024-07-01",
         1500000
        ],
        [
         "2024-07-02",
         1500000
        ],
        [
         "2024-07-03",
         1500000
        ],
        [
         "2024-07-04",
         1500000
        ],
        [
         "2024-07-05",
         1500000
        ],
        [
         "2024-07-06",
         1500000
        ],
        [
         "2024-07-07",
         1500000
        ],
        [
         "2024-07-08",
         1500000
        ],
        [
         "2024-07-09",
         1500000
        ],
        [
         "2024-07-10",
         1500000
        ],
        [
         "2024-07-11",
         1500000
        ],
        [
         "2024-07-12",
         1500000
        ],
        [
         "2024-07-13",
         1500000
        ],
        [
         "2024-07-14",
         1500000
        ],
        [
         "2024-07-15",
         1500000
        ],
        [
         "2024-07-16",
         1500000
        ],
        [
         "2024-07-17",
         1500000
        ],
        [
         "2024-07-18",
         1500000
        ],
        [
         "2024-07-19",
         1500000
        ],
        [
         "2024-07-20",
         1500000
        ],
        [
         "2024-07-21",
         1500000
        ],
        [
         "2024-07-22",
         1500000
        ],
        [
         "2024-07-23",
         1500000
        ],
        [
         "2024-07-24",
         1500000
        ],
        [
         "2024-07-25",
         1500000
        ],
        [
         "2024-07-26",
         1500000
        ],
        [
         "2024-07-27",
         1500000
        ],
        [
         "2024-07-28",
         1500000
        ],
        [
         "2024-07-29",
         1500000
        ],
        [
         "2024-07-30",
         1500000
        ],
        [
         "2024-08-01",
         1500000
        ],
        [
         "2024-08-02",
         1500000
        ],
        [
         "2024-08-03",
         1500000
        ],
        [
         "2024-08-04",
         1500000
        ],
        [
         "2024-08-05",
         1500000
        ],
        [
         "2024-08-06",
         1500000
        ],
        [
         "2024-08-07",
         1500000
        ],
        [
         "2024-08-08",
         1500000
        ],
        [
         "2024-08-09",
         1500000
        ],
        [
         "2024-08-10",
         1500000
        ],
        [
         "2024-08-11",
         1500000
        ],
        [
         "2024-08-12",
         1500000
        ],
        [
         "2024-08-13",
         1500000
        ],
        [
         "2024-08-14",
         1500000
        ],
        [
         "2024-08-15",
         1500000
        ],
        [
         "2024-08-16",
         1500000
        ],
        [
         "2024-08-17",
         1500000
        ],
        [
         "2024-08-18",
         1500000
        ],
        [
         "2024-08-19",
         1500000
        ],
        [
         "2024-08-20",
         1500000
        ],
        [
         "2024-08-21",
         1500000
        ],
        [
         "2024-08-22",
         1500000
        ],
        [
         "2024-08-23",
         1500000
        ],
        [
         "2024-08-24",
         1500000
        ],
        [
         "2024-08-25",
         1500000
        ],
        [
         "2024-08-26",
         1500000
        ],
        [
         "2024-08-27",
         1500000
        ],
        [
         "2024-08-28",
         1500000
        ],
        [
         "2024-08-29",
         1500000
        ],
        [
         "2024-08-30",
         1500000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ingest_day",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>qty</th><th>price</th><th>amount</th></tr></thead><tbody><tr><td>count</td><td>270000000</td><td>270000000</td><td>270000000</td></tr><tr><td>mean</td><td>3.4988466444444444</td><td>20.932150134442605</td><td>73.23765674844637</td></tr><tr><td>stddev</td><td>1.7087857024454163</td><td>5.994443161305232</td><td>42.71738401180422</td></tr><tr><td>min</td><td>1</td><td>12.18</td><td>12.18</td></tr><tr><td>25%</td><td>2</td><td>15.64</td><td>38.28</td></tr><tr><td>50%</td><td>3</td><td>20.09</td><td>66.5</td></tr><tr><td>75%</td><td>5</td><td>25.79</td><td>99.09</td></tr><tr><td>max</td><td>6</td><td>33.12</td><td>198.71999999999997</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count",
         "270000000",
         "270000000",
         "270000000"
        ],
        [
         "mean",
         "3.4988466444444444",
         "20.932150134442605",
         "73.23765674844637"
        ],
        [
         "stddev",
         "1.7087857024454163",
         "5.994443161305232",
         "42.71738401180422"
        ],
        [
         "min",
         "1",
         "12.18",
         "12.18"
        ],
        [
         "25%",
         "2",
         "15.64",
         "38.28"
        ],
        [
         "50%",
         "3",
         "20.09",
         "66.5"
        ],
        [
         "75%",
         "5",
         "25.79",
         "99.09"
        ],
        [
         "max",
         "6",
         "33.12",
         "198.71999999999997"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "qty",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tx_silver = spark.read.format(\"delta\").load(TX_SILVER_PATH)\n",
    "cust_silver = spark.read.format(\"delta\").load(CUST_SILVER_PATH)\n",
    "\n",
    "print(\"Silver tx rows:\", tx_silver.count())\n",
    "print(\"Silver cust rows:\", cust_silver.count())\n",
    "\n",
    "# Partition health\n",
    "display(tx_silver.groupBy(\"ingest_day\").count().orderBy(\"ingest_day\"))\n",
    "\n",
    "# Basic stats (useful to spot anomalies)\n",
    "display(tx_silver.select(\"qty\",\"price\",\"amount\").summary(\"count\",\"mean\",\"stddev\",\"min\",\"25%\",\"50%\",\"75%\",\"max\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9e91d33-e51f-435b-b573-eac1790a4cb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5221284682473625>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mselect max(ingest_day) from cust_bronze;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mselect max(ingest_day) from cust_silver\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2493\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n",
       "\u001B[1;32m   2491\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n",
       "\u001B[1;32m   2492\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n",
       "\u001B[0;32m-> 2493\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   2495\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n",
       "\u001B[1;32m   2496\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n",
       "\u001B[1;32m   2497\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n",
       "\u001B[1;32m   2498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:191\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    185\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mlogDriverEvent({\n",
       "\u001B[1;32m    186\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventType\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark-connect-sql-end\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    187\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventId\u001B[39m\u001B[38;5;124m\"\u001B[39m: comm\u001B[38;5;241m.\u001B[39mcomm_id,\n",
       "\u001B[1;32m    188\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBaseException\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    189\u001B[0m     })\n",
       "\u001B[1;32m    190\u001B[0m     comm\u001B[38;5;241m.\u001B[39msend({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m: traceback\u001B[38;5;241m.\u001B[39mformat_exc()})\n",
       "\u001B[0;32m--> 191\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m    193\u001B[0m     remove_control_channel_comm_handler(comm\u001B[38;5;241m.\u001B[39mcomm_id)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:141\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    139\u001B[0m maxNumberOfChoices \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaxNumberOfChoices\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister_udf(request\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mescaped_widget_values\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[0;32m--> 141\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_query_request_result(request[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[1;32m    143\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m request\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mschema) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:97\u001B[0m, in \u001B[0;36mSqlMagic.get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n",
       "\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_param_syntax_supported:\n",
       "\u001B[1;32m     96\u001B[0m     widget_bindings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()\n",
       "\u001B[0;32m---> 97\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n",
       "\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    100\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(query)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/session.py:736\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    733\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n",
       "\u001B[1;32m    735\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n",
       "\u001B[0;32m--> 736\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n",
       "\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n",
       "\u001B[1;32m    738\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1303\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1301\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1302\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1303\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1304\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1305\u001B[0m )\n",
       "\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1307\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1761\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1758\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1761\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1762\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1763\u001B[0m     ):\n",
       "\u001B[1;32m   1764\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1765\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2162\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2148\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2149\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2150\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2158\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m, default\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE),\n",
       "\u001B[1;32m   2159\u001B[0m                 ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2160\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2162\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2163\u001B[0m                 info,\n",
       "\u001B[1;32m   2164\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2165\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2166\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2167\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2169\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2170\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2171\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2172\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2173\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `cust_bronze` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 28;\n",
       "'Project [unresolvedalias('max('ingest_day))]\n",
       "+- 'UnresolvedRelation [cust_bronze], [], false\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
       "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:90)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:216)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:216)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:198)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:369)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:186)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:174)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:174)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:369)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:424)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:194)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:424)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:421)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:301)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:489)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:677)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:146)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:677)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1305)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:670)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:667)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:667)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:295)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:294)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:276)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:130)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1217)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1217)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:989)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:973)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3080)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2928)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2865)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:366)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:210)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:392)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:392)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:84)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:83)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:391)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:210)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:592)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:592)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `cust_bronze` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 28;\n'Project [unresolvedalias('max('ingest_day))]\n+- 'UnresolvedRelation [cust_bronze], [], false\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:216)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:216)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:198)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:369)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:174)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:174)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:369)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:424)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:194)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:424)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:421)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:301)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:677)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:146)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:677)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1305)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:670)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:667)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:667)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:295)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:294)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:276)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:130)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1217)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1217)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:989)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:973)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3080)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2928)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2865)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:366)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:210)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:392)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:392)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:84)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:83)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:391)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:210)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:592)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:592)"
       },
       "metadata": {
        "errorSummary": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `cust_bronze` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "TABLE_OR_VIEW_NOT_FOUND",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "42P01",
        "stackTrace": null,
        "startIndex": 28,
        "stopIndex": 38
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-5221284682473625>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mselect max(ingest_day) from cust_bronze;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mselect max(ingest_day) from cust_silver\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2493\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2491\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2492\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2493\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2495\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2496\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2497\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:191\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mlogDriverEvent({\n\u001B[1;32m    186\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventType\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark-connect-sql-end\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    187\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meventId\u001B[39m\u001B[38;5;124m\"\u001B[39m: comm\u001B[38;5;241m.\u001B[39mcomm_id,\n\u001B[1;32m    188\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBaseException\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    189\u001B[0m     })\n\u001B[1;32m    190\u001B[0m     comm\u001B[38;5;241m.\u001B[39msend({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m: traceback\u001B[38;5;241m.\u001B[39mformat_exc()})\n\u001B[0;32m--> 191\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    193\u001B[0m     remove_control_channel_comm_handler(comm\u001B[38;5;241m.\u001B[39mcomm_id)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:141\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    139\u001B[0m maxNumberOfChoices \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaxNumberOfChoices\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister_udf(request\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mescaped_widget_values\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 141\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_query_request_result(request[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    143\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m request\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mschema) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:97\u001B[0m, in \u001B[0;36mSqlMagic.get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_param_syntax_supported:\n\u001B[1;32m     96\u001B[0m     widget_bindings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()\n\u001B[0;32m---> 97\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    100\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(query)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/session.py:736\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m    733\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n\u001B[1;32m    735\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001B[0;32m--> 736\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m    738\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1303\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1301\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1302\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1303\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1304\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1305\u001B[0m )\n\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1307\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1761\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1758\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1761\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1762\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1763\u001B[0m     ):\n\u001B[1;32m   1764\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1765\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2162\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2148\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2149\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2150\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2158\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m, default\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE),\n\u001B[1;32m   2159\u001B[0m                 ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2160\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2162\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2163\u001B[0m                 info,\n\u001B[1;32m   2164\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2165\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2166\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2167\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2169\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2170\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2171\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2172\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2173\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `cust_bronze` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 28;\n'Project [unresolvedalias('max('ingest_day))]\n+- 'UnresolvedRelation [cust_bronze], [], false\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:90)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:216)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:293)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:292)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:216)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:198)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:369)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:186)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:174)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:174)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:369)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:424)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:194)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:424)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:421)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:301)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:489)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:677)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:146)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:677)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1305)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:670)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:667)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:667)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:295)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:294)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:276)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:130)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1217)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1217)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:989)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:973)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3080)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2928)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2865)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:366)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:210)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:392)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:392)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:84)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:83)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:391)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:210)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:592)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:592)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select max(ingest_day) from cust_bronze;\n",
    "select max(ingest_day) from cust_silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d98bb9-2e24-46a4-bdc3-d5f60a9c15ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5221284682473625,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Benching_silver.py",
   "widgets": {
    "DATE_FROM": {
     "currentValue": "2024-01-11",
     "nuid": "d1927096-dac4-4415-8ec1-f7812a4efdd8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "DATE_FROM",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "DATE_FROM",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "DATE_TO": {
     "currentValue": "2024-05-31",
     "nuid": "5b840011-46a9-4003-85e5-a0d078f34f5a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "DATE_TO",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "DATE_TO",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "WRITE_MODE": {
     "currentValue": "overwrite_range",
     "nuid": "ea8574fe-3803-4908-8021-0e2b2531c3d0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "append",
      "label": null,
      "name": "WRITE_MODE",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "append",
        "overwrite_full",
        "overwrite_range"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "append",
      "label": null,
      "name": "WRITE_MODE",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "append",
        "overwrite_full",
        "overwrite_range"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}